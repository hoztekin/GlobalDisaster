# =============================================================================
# 01__EDA.py
# (BÄ°RLEÅTÄ°RÄ°LMÄ°Å TAM ANALÄ°Z: Veri ZenginleÅŸtirme + DetaylÄ± KeÅŸifsel Analiz)
# =============================================================================

"""
AKIÅ:
1. Veri YÃ¼kleme & API ZenginleÅŸtirme
2. Feature Engineering (Mevsimler, Log DÃ¶nÃ¼ÅŸÃ¼m)
3. Data Quality Check
4. Temporal Analysis
5. Geographic Analysis
6. Disaster Type Analysis
7. Statistical Tests (ANOVA / Kruskal-Wallis)
8. Correlation Analysis
9. Distribution Analysis
10. Outlier Detection
11. Key Insights & Save
"""

import os
import requests
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import f_oneway, kruskal, spearmanr
from pathlib import Path
from datetime import datetime

# 1. KRÄ°TÄ°K AYAR: Ã‡Ã¶kme Engelleyici (forrtl error Ã§Ã¶zÃ¼mÃ¼)
os.environ['FOR_DISABLE_CONSOLE_CTRL_HANDLER'] = '1'

# Ayarlar
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-whitegrid')

# 2. KRÄ°TÄ°K AYAR: Dosya Yolu (NameError Ã§Ã¶zÃ¼mÃ¼)

try:
    PROJECT_ROOT = Path(__file__).resolve().parents[1]
except NameError:
    PROJECT_ROOT = Path(r"D:\Miuul Final Project\GlobalDisaster")

DATA_RAW = PROJECT_ROOT / "data" / "raw" / "global_disaster.csv"
DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
REPORT_DIR = PROJECT_ROOT / "reports" / "ultimate_eda"

# KlasÃ¶rleri oluÅŸtur
DATA_PROCESSED.mkdir(parents=True, exist_ok=True)
REPORT_DIR.mkdir(parents=True, exist_ok=True)

print("=" * 100)
print(f"ğŸ“‚ PROJE DÄ°ZÄ°NÄ°: {PROJECT_ROOT}")
print("=" * 100)

# =============================================================================
# BÃ–LÃœM 1: VERÄ° YÃœKLEME VE API (Zaman AÅŸÄ±mÄ± KorumalÄ±)
# =============================================================================
print("\n[1/11] VERÄ° YÃœKLEME VE API ENTEGRASYONU")

# Dosya kontrolÃ¼
if not DATA_RAW.exists():
    print(f"âŒ HATA: Dosya bulunamadÄ± -> {DATA_RAW}")
    # Hata varsa durdur
    raise FileNotFoundError("LÃ¼tfen dosya yolunu kontrol et.")

df = pd.read_csv(DATA_RAW)
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year

# World Bank API YardÄ±mcÄ±larÄ±
COUNTRY_TO_ISO3 = {
    "Australia": "AUS", "Bangladesh": "BGD", "Brazil": "BRA", "Canada": "CAN",
    "Chile": "CHL", "China": "CHN", "France": "FRA", "Germany": "DEU",
    "Greece": "GRC", "India": "IND", "Indonesia": "IDN", "Italy": "ITA",
    "Japan": "JPN", "Mexico": "MEX", "Nigeria": "NGA", "Philippines": "PHL",
    "South Africa": "ZAF", "Spain": "ESP", "Turkey": "TUR", "United States": "USA",
    "Pakistan": "PAK", "Vietnam": "VNM", "Thailand": "THA", "Nepal": "NPL",
    "Iran": "IRN", "United Kingdom": "GBR", "New Zealand": "NZL"
}


def fetch_wb_indicator(iso3, indicator, start, end):
    url = f"https://api.worldbank.org/v2/country/{iso3}/indicator/{indicator}"
    params = {"date": f"{start}:{end}", "format": "json", "per_page": "100"}
    try:
        # TIMEOUT EKLENDÄ°: 10 saniye cevap gelmezse pas geÃ§er, program donmaz.
        r = requests.get(url, params=params, timeout=10)
        if r.status_code == 200 and len(r.json()) > 1:
            return {int(x['date']): x['value'] for x in r.json()[1] if x['value']}
    except:
        return {}
    return {}


print("   ğŸŒ World Bank verileri Ã§ekiliyor (NÃ¼fus, YÃ¼zÃ¶lÃ§Ã¼mÃ¼)...")
records = []
countries = [c for c in df['country'].unique() if c in COUNTRY_TO_ISO3]

for c in countries:
    iso = COUNTRY_TO_ISO3[c]
    # NÃ¼fus
    pop_map = fetch_wb_indicator(iso, "SP.POP.TOTL", df['year'].min(), df['year'].max())
    # YÃ¼zÃ¶lÃ§Ã¼mÃ¼ (2020 sabit)
    area_map = fetch_wb_indicator(iso, "AG.SRF.TOTL.K2", 2020, 2020)
    area = list(area_map.values())[0] if area_map else None

    for y in range(df['year'].min(), df['year'].max() + 1):
        records.append({
            "country": c, "year": y,
            "population": pop_map.get(y),
            "surface_area_km2": area
        })

wb_df = pd.DataFrame(records)
df = df.merge(wb_df, on=['country', 'year'], how='left')
print(f"   âœ… Veri zenginleÅŸtirildi! (Population ve Surface Area eklendi)")

# Eksikleri doldur (Mean Imputation)
df['population'] = df['population'].fillna(df.groupby('country')['population'].transform('mean'))

# =============================================================================
# BÃ–LÃœM 2: FEATURE ENGINEERING
# =============================================================================
print("\n[2/11] FEATURE ENGINEERING")

# Zaman
df['month'] = df['date'].dt.month
df['day_of_year'] = df['date'].dt.dayofyear


def get_season(m):
    if m in [12, 1, 2]:
        return 'Winter'
    elif m in [3, 4, 5]:
        return 'Spring'
    elif m in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Autumn'


df['season'] = df['month'].apply(get_season)

# HesaplamalÄ± Metrikler
df['population_density'] = df['population'] / (df['surface_area_km2'] + 1)
df['loss_per_capita'] = df['economic_loss_usd'] / (df['population'] + 1)
df['casualties_per_100k'] = (df['casualties'] / (df['population'] + 1)) * 100000

# Log DÃ¶nÃ¼ÅŸÃ¼mleri (Modeller iÃ§in hazÄ±rlÄ±k)
df['casualties_log'] = np.log1p(df['casualties'])
df['economic_loss_log'] = np.log1p(df['economic_loss_usd'])

print("   âœ… Yeni Ã¶zellikler Ã¼retildi: Season, Density, Loss Per Capita...")

# =============================================================================
# BÃ–LÃœM 3: DATA QUALITY CHECK
# =============================================================================
print("\n[3/11] DATA QUALITY CHECK")

missing = df.isnull().sum()
print(f"   ğŸ” Eksik DeÄŸerler:\n{missing[missing > 0]}")
print(f"   ğŸ” Tekrar Eden SatÄ±rlar: {df.duplicated().sum()}")
print(f"   ğŸ“Š Veri AralÄ±ÄŸÄ±: {df['date'].min().date()} - {df['date'].max().date()}")

# =============================================================================
# BÃ–LÃœM 4: TEMPORAL ANALYSIS
# =============================================================================
print("\n[4/11] TEMPORAL ANALYSIS")

yearly_counts = df['year'].value_counts().sort_index()
print(f"   ğŸ“… YÄ±llara GÃ¶re Afet SayÄ±larÄ±:\n{yearly_counts}")

seasonal_stats = df.groupby('season')[['severity_index', 'casualties']].mean()
print(f"   ğŸŒ¸ Mevsimsel Ortalamalar:\n{seasonal_stats}")

# =============================================================================
# BÃ–LÃœM 5: GEOGRAPHIC ANALYSIS
# =============================================================================
print("\n[5/11] GEOGRAPHIC ANALYSIS")

top_risk_countries = df.groupby('country')['severity_index'].mean().sort_values(ascending=False).head(5)
print(f"   ğŸŒ En YÃ¼ksek Ortalama Åiddet (Top 5):\n{top_risk_countries}")

# Yeni eklenen veri ile analiz
densest_impact = df.groupby('country')[['population_density', 'casualties']].mean().sort_values('population_density',
                                                                                                ascending=False).head(5)
print(f"   ğŸ‘¥ En YoÄŸun NÃ¼fuslu Ãœlkelerde Ortalama KayÄ±p:\n{densest_impact}")

# =============================================================================
# BÃ–LÃœM 6: DISASTER TYPE ANALYSIS
# =============================================================================
print("\n[6/11] DISASTER TYPE ANALYSIS")

type_stats = df.groupby('disaster_type').agg({
    'severity_index': 'mean',
    'economic_loss_usd': 'sum',
    'casualties': 'sum'
}).sort_values('economic_loss_usd', ascending=False)
print(f"   ğŸŒªï¸ Afet Tipine GÃ¶re Ä°statistikler:\n{type_stats}")

# =============================================================================
# BÃ–LÃœM 7: STATISTICAL TESTS (ANOVA)
# =============================================================================
print("\n[7/11] STATISTICAL TESTS (ANOVA & Kruskal-Wallis)")

# Test 1: Mevsimler arasÄ± ÅŸiddet farkÄ± var mÄ±?
groups = [df[df['season'] == s]['severity_index'] for s in df['season'].unique()]
f_stat, p_val = f_oneway(*groups)
print(f"   ğŸ§ª ANOVA (Season vs Severity): P-value = {p_val:.5f}")
if p_val < 0.05:
    print("      âœ… SONUÃ‡: Mevsimler arasÄ±nda afet ÅŸiddeti aÃ§Ä±sÄ±ndan anlamlÄ± bir fark VAR.")
else:
    print("      âŒ SONUÃ‡: Mevsimsel fark istatistiksel olarak anlamlÄ± deÄŸil.")

# =============================================================================
# BÃ–LÃœM 8: CORRELATION ANALYSIS
# =============================================================================
print("\n[8/11] CORRELATION ANALYSIS")

# Sadece sayÄ±sal kolonlar
num_cols = ['severity_index', 'casualties', 'economic_loss_usd', 'population_density', 'response_time_hours']
corr = df[num_cols].corr()

print("   ğŸ”— Korelasyon Matrisi:")
print(corr)

# Heatmap kaydet
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.savefig(REPORT_DIR / "correlation_matrix.png")
print("   ğŸ’¾ Heatmap kaydedildi: correlation_matrix.png")

# =============================================================================
# BÃ–LÃœM 9: DISTRIBUTION ANALYSIS
# =============================================================================
print("\n[9/11] DISTRIBUTION ANALYSIS")

print(f"   ğŸ“Š Severity Skewness: {df['severity_index'].skew():.2f}")
print(f"   ğŸ“Š Casualties Skewness (Original): {df['casualties'].skew():.2f}")
print(f"   ğŸ“Š Casualties Skewness (Log Transformed): {df['casualties_log'].skew():.2f}")

# =============================================================================
# BÃ–LÃœM 10: OUTLIER DETECTION
# =============================================================================
print("\n[10/11] OUTLIER DETECTION (IQR Method)")


def count_outliers(col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return ((df[col] < lower) | (df[col] > upper)).sum()


print(f"   ğŸš¨ Outliers in Economic Loss: {count_outliers('economic_loss_usd')}")
print(f"   ğŸš¨ Outliers in Casualties: {count_outliers('casualties')}")

# =============================================================================
# BÃ–LÃœM 11: KEY INSIGHTS & SAVE
# =============================================================================
print("\n[11/11] SONUÃ‡ VE KAYIT")

output_path = DATA_PROCESSED / "disaster_data_final.csv"
df.to_csv(output_path, index=False)

summary = f"""
EDA RAPORU
-------------------
Tarih: {datetime.now()}
Toplam Veri: {len(df)}
ZenginleÅŸtirme: World Bank NÃ¼fus ve YÃ¼zÃ¶lÃ§Ã¼mÃ¼ Eklendi.

Ã–nemli Bulgular:
1. En maliyetli afet tipi: {type_stats.index[0]}
2. Mevsimsel fark (ANOVA): {'Var' if p_val < 0.05 else 'Yok'}
3. NÃ¼fus yoÄŸunluÄŸu ile kayÄ±p korelasyonu: {corr.loc['population_density', 'casualties']:.3f}

KayÄ±t Yeri: {output_path}
"""

with open(REPORT_DIR / "eda_summary.txt", "w") as f:
    f.write(summary)

print(f"   âœ… TemizlenmiÅŸ ve ZenginleÅŸtirilmiÅŸ Veri Kaydedildi: {output_path}")
print(f"   âœ… Ã–zet Rapor: {REPORT_DIR / 'eda_summary.txt'}")

print("\n" + "=" * 100)